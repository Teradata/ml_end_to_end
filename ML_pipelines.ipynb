{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc96f5f-f364-43df-801f-ba69152e2a70",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\">\n",
    "<img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>End to End ML Pipelines with Teradata Vantage</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb58c1-f0df-4ed7-9412-04d2af620a1c",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Introduction</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this notebook we explore a hypothetical end to end generative AI pipeline to illustrate the usage of different tools offered by Teradata Vantage to easily, define a problem, collect, clean and preprocess data, integrate training data to cloud native machine learning tools, and operationalize the trained model. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Contents</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Problem Definition</li>\n",
    "    <li>Loading of Sample Data</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>Data Cleaning and Preprocessing</li>\n",
    "    <li>Model Training</li>\n",
    "    <li>Operationalization</li>\n",
    "</ol>\n",
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Notes</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>This notebook is designed to work, with minimum extra configuration, within a ClearScape Analytics Experience environment, it can be added into a separate folder in the UseCases environment. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b52974-2198-4e85-a6bd-b193cc25e5da",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Importing Teradata Machine Learning Libraries</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>These libraries are already available within ClearScape Analytics Experience.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecfb4d-88f7-4c02-a04f-f90582478c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import teradataml as tdml\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import tdnpathviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26cb94",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Establish Database Connection</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Set up the connection, at this moment there is no database defined in the connection, the default database is created as the first step below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b993a2cc-d7b5-49e1-ab2f-d26f9f2f3b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing setup ...\n",
      "Setup complete\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter password:  ··········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logon successful\n",
      "Connected as: teradatasql://demo_user:xxxxx@host.docker.internal/dbc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/teradataml/context/context.py:481: TeradataMlRuntimeWarning: Warning: Password is URL encoded.\n",
      "  warnings.warn(\"Warning: Password is URL encoded.\", category=TeradataMlRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = tdml.create_context(host = 'host.docker.internal', username='demo_user', password = password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e79bf3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Create default database</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907ca38-b318-49e2-b828-b8c14e26fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "CREATE DATABASE teddy_retailers_ml\n",
    "AS PERMANENT = 110e6;\n",
    "\"\"\"\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d805ae",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Define the default database in the context</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af106ec4-37cc-4b37-bd8d-57b0c71aaf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/teradataml/context/context.py:481: TeradataMlRuntimeWarning: Warning: Password is URL encoded.\n",
      "  warnings.warn(\"Warning: Password is URL encoded.\", category=TeradataMlRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "eng = tdml.create_context(host = 'host.docker.internal', username='demo_user', password = password, database = 'teddy_retailers_ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9e245",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Creation of the tables with sample mock data, these tables are created by reading information from object storage</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ddaf38-d0fa-4ff3-93f8-3387cfc618e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7ff3a8119280>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry='''\n",
    "CREATE MULTISET TABLE teddy_retailers_ml.products AS\n",
    "(\n",
    "  SELECT product_id, product_name, department_id\n",
    "    FROM (\n",
    "\t\tLOCATION='/gs/storage.googleapis.com/clearscape_analytics_demo_data/DEMO_groceryML/products.csv') as products\n",
    ") WITH DATA;\n",
    "'''\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d19de-a7ca-4536-a491-40acd7687dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7ff376997eb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry='''\n",
    "CREATE MULTISET TABLE teddy_retailers_ml.order_products AS\n",
    "(\n",
    "  SELECT order_id, product_id, add_cart_order\n",
    "    FROM (\n",
    "\t\tLOCATION='/gs/storage.googleapis.com/clearscape_analytics_demo_data/DEMO_groceryML/order_products.csv') as orders_products\n",
    ") WITH DATA;\n",
    "'''\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13a4d9",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Data Exploration</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>At this stage we load data from the database to Teradata DataFrames. This allows to perform analysis with ease</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac2016-076a-40e4-b82b-dfdef8d04106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable {border:ridge 5px;}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}</style>\n",
       "<html><table>\n",
       "\t<tr id=\"HeaderRow\">\n",
       "\t\t<th>order_id</th>\n",
       "\t\t<th>product_id</th>\n",
       "\t\t<th>add_cart_order</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>180</td>\n",
       "\t\t<td>4</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>8</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>140</td>\n",
       "\t\t<td>6</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>17</td>\n",
       "\t\t<td>14</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>11</td>\n",
       "\t\t<td>13</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>14</td>\n",
       "\t\t<td>5</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>53</td>\n",
       "\t\t<td>11</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>153</td>\n",
       "\t\t<td>10</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>39</td>\n",
       "\t\t<td>3</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>142</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "          product_id  add_cart_order\n",
       "order_id                            \n",
       "3487             130              15\n",
       "3487             120              13\n",
       "3487              43               3\n",
       "3487              13              11\n",
       "3487             190               5\n",
       "3487             159               6\n",
       "3487              42               2\n",
       "3487              11              12\n",
       "3487              22              16\n",
       "3487              38               8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = tdml.DataFrame(\"order_products\")\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115d970",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The statement below generates a DataFrame that aggregates the number of products added to each order</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e6780-4094-4fb9-a8c2-e3e3b2cc145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_order = orders.groupby(\"order_id\").agg({\"product_id\": \"count\"})\n",
    "counts_per_order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f32f3d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>With the DataFrame that was created above, it is possible to create a histogram DataFrame, a histogram plot can be generated with base on this Dataframe.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba222c-ee9b-4fb1-bbbd-beff54c296dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_prod_hist = tdml.Histogram(data=counts_per_order,\n",
    "                target_columns=\"count_product_id\",\n",
    "                method_type=\"Sturges\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc6908-6858-4cdd-83e9-f30988851a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_prod_hist_pd = count_prod_hist.result.sort(\"Label\").to_pandas()\n",
    "count_prod_hist_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9ccdf-0dd7-4dbe-a22e-1cdd434a7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(count_prod_hist_pd, x=\"MaxValue\", y=\"Bin_Percent\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcfa284",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As we ingested the factual data regarding orders, we can also add dimmensional/categorical data regarding products, we can retrieve the product names from this dataset, this is useful for illustrative purposes below </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc4c915-4a39-41ae-96ba-b0246e05d8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable {border:ridge 5px;}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}</style>\n",
       "<html><table>\n",
       "\t<tr id=\"HeaderRow\">\n",
       "\t\t<th>product_id</th>\n",
       "\t\t<th>product_name</th>\n",
       "\t\t<th>department_id</th>\n",
       "\t\t<th>seq_product_id</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>59</td>\n",
       "\t\t<td>Chicken</td>\n",
       "\t\t<td>3</td>\n",
       "\t\t<td>161</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>160</td>\n",
       "\t\t<td>AluminumPans</td>\n",
       "\t\t<td>4</td>\n",
       "\t\t<td>262</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>36</td>\n",
       "\t\t<td>Pepper</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>138</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>97</td>\n",
       "\t\t<td>All-PurposeCleaner</td>\n",
       "\t\t<td>4</td>\n",
       "\t\t<td>199</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>137</td>\n",
       "\t\t<td>ColdMedicine</td>\n",
       "\t\t<td>6</td>\n",
       "\t\t<td>239</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>177</td>\n",
       "\t\t<td>ExtensionCords</td>\n",
       "\t\t<td>4</td>\n",
       "\t\t<td>279</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>15</td>\n",
       "\t\t<td>TomatoSauce</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>117</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>99</td>\n",
       "\t\t<td>OvenCleaner</td>\n",
       "\t\t<td>4</td>\n",
       "\t\t<td>201</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>19</td>\n",
       "\t\t<td>CannedVegetables</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>121</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>122</td>\n",
       "\t\t<td>Antiperspirant</td>\n",
       "\t\t<td>5</td>\n",
       "\t\t<td>224</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "                     product_name  department_id  seq_product_id\n",
       "product_id                                                      \n",
       "17                           Rice              2             119\n",
       "179                 LightFixtures              4             281\n",
       "34                     BakingSoda              1             136\n",
       "156         FoodStorageContainers              4             258\n",
       "196                       Gaskets              4             298\n",
       "32                          Flour              1             134\n",
       "13                          Jelly              1             115\n",
       "139                 PainRelievers              6             241\n",
       "141                      Vitamins              6             243\n",
       "101                       Sponges              4             203"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = tdml.DataFrame('products')\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90626632",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>After ingesting the product categorical data, we can merge the categorical data and factual data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_products_merged = orders.join(\n",
    "    other = products,\n",
    "    on = \"product_id\",\n",
    "    how = \"inner\",\n",
    "    lsuffix = \"ordrs\", \n",
    "    rsuffix = \"prdt\")\n",
    "orders_products_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64647ac6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>With this merged table we can analyze what are the most commonly ordered products</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Another standard exploratory step involves identifying the most popular products and the most frequent sequences in which these products are added to the shopping cart.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea02f4-4e64-4be6-af95-5119437f994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_counts = orders_products_merged.groupby('product_name').agg({\"ordrs_product_id\": \"count\"})\n",
    "product_counts.sort(\"count_ordrs_product_id\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19149d3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>To identify the most common sequences in which products are added to the shopping cart, we need to identify the sequences in which products were included on each of the order. For ease of analysis, we'll express these sequences as pairs. This implies we'll identify pairs of products in which one product frequently follows another.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The Teradata nPath function is routinely used for such purposes. This function is particularly useful for identifying sequential patterns within data. The primary inputs for nPath are the rows to be analyzed, the column used for data partitioning, and the column that dictates the order of the sequence.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In our case, we'll perform the path analysis on the data in `orders_products_merged`. Since we aim to identify sequential patterns in relations to orders, our partition column is `order_id`. Given that we want to construct the sequence based on the order in which products were added to the cart, our ordinal column is `add_cart_order`.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Other key components of the nPath function include the conditions that a data point should meet to be included in the sequence, and the patterns we're identifying among the rows that meet the condition. These are supplied to nPath as the parameters `symbol` and `pattern`, respectively.\n",
    "The nPath function for our purpose would be as follows:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de74fd9-957b-4669-8c58-39e9b5800d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_seqs = tdml.NPath(\n",
    "    data1=orders_products_merged,\n",
    "    data1_partition_column=\"order_id\",\n",
    "    data1_order_column=\"add_cart_order\",\n",
    "    mode=\"OVERLAPPING\",\n",
    "    pattern=\"A.A\",\n",
    "    symbols=\"TRUE as A\",\n",
    "    result=[\n",
    "        \"FIRST (order_id OF A) AS order_id\",\n",
    "        \"ACCUMULATE (product_name OF A) AS path\",\n",
    "        \"COUNT (* OF A) AS countrank\"\n",
    "        ]\n",
    "    ).result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcbfa6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Where the symbol `A` merely implies that a `product_id` `A` exist in an order, thus `True as A`. The pattern `A.A` is fulfilled each time a given product `A`, follows another product `A` added previously in the shopping cart. We set the mode as overlapping since the second element of a given pair could be the first element of another pair. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The `FIRST` function retrieves the `order_id` from the initial row that matched the pattern, while ACCUMULATE constructs the path with the `product_name` found in every matched row, following the sequence defined by `add_cart_order`.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdcc89",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>To obtain a count of the most common pairs, we group by 'path' and aggregate by the count of 'order_id', sorting the results in descending order based on the count. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98803ee0-f965-4dfe-a6c0-577905699d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_seqs.groupby('path').agg({\"order_id\": \"count\"}).sort('count_order_id',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200f60b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The main paths found through this analysis can be plotted through Teradata's tdnpathviz visualization module.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb90a8-e10d-4511-98ee-be7ff14e3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import plot_first_main_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fd41a-092a-48a2-be40-34205add79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_main_paths(common_seqs,path_column='path',id_column='order_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f74b65",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Data Cleaning and Preprocessing</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To prepare the data for processing by a machine learning library or a pre-trained model, especially those required for generative AI through NLP, we need to verify a few things:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Product identifiers need to be sequential. This ensures that the NLP model can process them as it would do with words in a sentence. As well we must define markers to accurately signify the start and end of a “sentence”, similar to a capital letter and a period in English grammar.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We need to eliminate null values. Machine learning training via NLP tools fundamentally involves matrix multiplication, which can only be performed with numerical values.\n",
    "Adding sequential product identifiers:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In our specific dataset, the product identifier `product_id` happens to be sequential, however, this is just a matter of chance, not design, for this reason we are going to assume that the `product_id` is not sequential, this has the extra advantage of allowing us to define the starting point of the sequence, which in turn we can utilize to define our “sentence” `start` and “sentence” `end` markers. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We are going to define the number `101` as our “sentence” `start` marker and the number `102` as our “sentence” end marker. With these two constants defined we are going to assign a sequential identifier to the items in our `products` table, this sequential identifier will start at `103` and continue from there. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dc87c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We create a Volatile Table `temp` that matches each product `product_id` with the sequential identifier that we are creating. For this we use SQL’s `ROW_NUMBER()` and `OVER` statements. This table will have a primary index on `product_id` we want this table to be preserved with the session, so we define the parameter `ON_COMMIT_PRESERVE_ROWS`:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb11ef-e751-42d8-bbe0-5181e2b7efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7ff376865040>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_qry = '''\n",
    "CREATE VOLATILE TABLE temp AS (\n",
    "    SELECT product_id, \n",
    "    ROW_NUMBER() OVER (ORDER BY product_id) + 102 as seq_product_id\n",
    "    FROM products\n",
    ") WITH DATA PRIMARY INDEX (product_id) ON COMMIT PRESERVE ROWS;\n",
    "'''\n",
    "eng.execute(create_table_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8d270",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>To create the sequential `product_id` this statement is retrieving the row number of each record in the `products` table, ordered by `product_id`, and adding 102 to that number, then the result is defined as `seq_product_id`.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f2ec4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We create a new column in our `products` table to store the sequential product identifiers:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bca0f-a868-47a4-9b31-ddab9d641d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7ff37689bdc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_column_qry = '''\n",
    "ALTER TABLE products\n",
    "ADD seq_product_id INTEGER;\n",
    "'''\n",
    "eng.execute(add_column_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3928667",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Finally, we add the sequential identifiers to our product’s table, as `seq_product_id`:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a46ff7-fd78-4327-bcf5-23d073ed6c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7ff376865a60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_table_qry = '''\n",
    "UPDATE products\n",
    "SET seq_product_id = (\n",
    "    SELECT temp.seq_product_id\n",
    "    FROM temp\n",
    "    WHERE products.product_id = temp.product_id\n",
    ");\n",
    "'''\n",
    "eng.execute(modify_table_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8a041",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The resulting table can be ingested again into our `products` `DataFrame`.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1c9d5b2-2eb0-4cb4-875e-201bd7957a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable {border:ridge 5px;}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}</style>\n",
       "<html><table>\n",
       "\t<tr id=\"HeaderRow\">\n",
       "\t\t<th>product_id</th>\n",
       "\t\t<th>product_name</th>\n",
       "\t\t<th>department_id</th>\n",
       "\t\t<th>seq_product_id</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>Milk</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>103</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>Bread</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>104</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3</td>\n",
       "\t\t<td>Eggs</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>105</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4</td>\n",
       "\t\t<td>Butter</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>106</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>5</td>\n",
       "\t\t<td>Cheese</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>107</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>6</td>\n",
       "\t\t<td>Yogurt</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>108</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>7</td>\n",
       "\t\t<td>Cereal</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>109</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>8</td>\n",
       "\t\t<td>Oatmeal</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>110</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>9</td>\n",
       "\t\t<td>GranolaBars</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>111</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>10</td>\n",
       "\t\t<td>PancakeMix</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>112</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "           product_name  department_id  seq_product_id\n",
       "product_id                                            \n",
       "1                  Milk              1             103\n",
       "2                 Bread              1             104\n",
       "3                  Eggs              1             105\n",
       "4                Butter              1             106\n",
       "5                Cheese              1             107\n",
       "6                Yogurt              1             108\n",
       "7                Cereal              1             109\n",
       "8               Oatmeal              1             110\n",
       "9           GranolaBars              1             111\n",
       "10           PancakeMix              1             112"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = tdml.DataFrame('products')\n",
    "products.sort('seq_product_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42ce3",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Tokenization and sequence tagging:</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In predicting the next product, a customer will add to their shopping cart, we interpret the sequence of previously added products as \"sentences\", where each product stands as a distinct \"word\". Widely used pre-trained NLP models, such as BERT for instance, necessitate a specific input format. This is where tokenization and tagging come into the picture. For the training stage we need to supply a dataset in which each order, or \"sentence\", is represented by the sequence of products or \"words\" added to it, including markers for both the beginning and end of the sequence.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>As part of our data preparation process, we will create sequences of products added per order as we did during the data exploration phase. However, we'll use sequential identifiers rather than product names in the `path`. These sequential identifiers serve as tokens to identify the products. As previously mentioned, we will also integrate markers for the beginning and end of the sequence.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c00ab",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>First, we need to recreate the `orders_products_merged` `DataFrame`, this time with the updated ‘products’ `DataFrame` in the join. We keep all the columns on the `products` `DataFrame` for reference and simplicity, in a real scenario we would keep only the id’s. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0bdfb-c056-40ee-825d-50462912b303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable {border:ridge 5px;}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}</style>\n",
       "<html><table>\n",
       "\t<tr id=\"HeaderRow\">\n",
       "\t\t<th>order_id</th>\n",
       "\t\t<th>ordrs_product_id</th>\n",
       "\t\t<th>prdt_product_id</th>\n",
       "\t\t<th>add_cart_order</th>\n",
       "\t\t<th>product_name</th>\n",
       "\t\t<th>department_id</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3752</td>\n",
       "\t\t<td>125</td>\n",
       "\t\t<td>125</td>\n",
       "\t\t<td>3</td>\n",
       "\t\t<td>Lotion</td>\n",
       "\t\t<td>5</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3487</td>\n",
       "\t\t<td>22</td>\n",
       "\t\t<td>22</td>\n",
       "\t\t<td>16</td>\n",
       "\t\t<td>Crackers</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3487</td>\n",
       "\t\t<td>130</td>\n",
       "\t\t<td>130</td>\n",
       "\t\t<td>15</td>\n",
       "\t\t<td>HairSpray</td>\n",
       "\t\t<td>5</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4425</td>\n",
       "\t\t<td>50</td>\n",
       "\t\t<td>50</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>FrozenPizza</td>\n",
       "\t\t<td>3</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4425</td>\n",
       "\t\t<td>76</td>\n",
       "\t\t<td>76</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>CatFood</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>142</td>\n",
       "\t\t<td>142</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>FirstAidSupplies</td>\n",
       "\t\t<td>6</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>39</td>\n",
       "\t\t<td>39</td>\n",
       "\t\t<td>3</td>\n",
       "\t\t<td>Vinegar</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3018</td>\n",
       "\t\t<td>180</td>\n",
       "\t\t<td>180</td>\n",
       "\t\t<td>4</td>\n",
       "\t\t<td>Brooms</td>\n",
       "\t\t<td>4</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4425</td>\n",
       "\t\t<td>86</td>\n",
       "\t\t<td>86</td>\n",
       "\t\t<td>8</td>\n",
       "\t\t<td>FacialTissues</td>\n",
       "\t\t<td>4</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>3487</td>\n",
       "\t\t<td>38</td>\n",
       "\t\t<td>38</td>\n",
       "\t\t<td>8</td>\n",
       "\t\t<td>CookingOil</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "          ordrs_product_id  prdt_product_id  add_cart_order      product_name  department_id\n",
       "order_id                                                                                    \n",
       "4425                    76               76               1           CatFood              1\n",
       "3487                    22               22              16          Crackers              1\n",
       "3487                   130              130              15         HairSpray              5\n",
       "469                    188              188               1         AllenKeys              4\n",
       "3752                   125              125               3            Lotion              5\n",
       "3018                   142              142               1  FirstAidSupplies              6\n",
       "3018                    39               39               3           Vinegar              1\n",
       "3018                   180              180               4            Brooms              4\n",
       "3752                   161              161               9          FoilPans              4\n",
       "3487                    38               38               8        CookingOil              1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_products_merged = orders.join(\n",
    "    other = products,\n",
    "    on = \"product_id\",\n",
    "    how = \"inner\",\n",
    "    lsuffix = \"ordrs\", \n",
    "    rsuffix = \"prdt\")\n",
    "orders_products_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010ced1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>From this modified `orders_products_merged` we will generate a preliminary `DataFrame` with the orders represented as sequences of products. At this stage each record will have at least two items. In cases in which only one item was added to the shopping cart, for example, the record would be populated with the beginning of sentence marker, and the sequential identifier of the product added.  </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To achieve this objective, we will add a new column to the `orders_products_merged` `DataFrame` with the constant value `101` which is our beginning of sentence marker.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d71ad2-fe86-4a5f-9a08-5ae3927fa835",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_products_merged = orders_products_merged.assign(\n",
    "    bgn = 101\n",
    ").select([\"order_id\", \"add_cart_order\", \"seq_product_id\", \"bgn\"])\n",
    "orders_products_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd16b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>With this updated `DataFrame`, we'll use Teradata’s nPath function to generate the sequences of products per order. This process is quite similar to what we did in the data exploration stage. The difference here is that we won't include the `order_id` as it is not part of the \"sentence\" and not relevant for training a model. Also, we won't accumulate all tokens in one column; instead, we'll have a distinct column for each token. Additionally, the pattern will not be looking for matches on product pairs; we'll consider any sequence, even if it only contains one product. For this, we use the wildcard `*`. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Another useful data point we identified during data exploration comes in handy here. Since we need to define a column for each token, and we aim to optimize column generation by adding columns where tokens are present, we'll rely on our earlier findings that the maximum number of products in an order in our dataset is `21`. We'll need at least one more column to account for an end-of-sentence marker in the case of longer lists, so we'll have 25 columns in our prepared dataset. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Carrying out all the processes above is made significantly easier with Teradata’s nPath function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd69905-e941-4448-9500-4286bf5a19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = tdml.NPath(\n",
    "    data1=orders_products_merged,\n",
    "    data1_partition_column=\"order_id\",\n",
    "    data1_order_column=\"add_cart_order\",\n",
    "    mode=\"NONOVERLAPPING\",\n",
    "    pattern=\"A*\",\n",
    "    symbols=\"TRUE as A\",\n",
    "    result=[\"FIRST (bgn OF A) AS c0\",\n",
    "            \"NTH (seq_product_id, 1 OF A) as c1\",\n",
    "            \"NTH (seq_product_id, 2 OF A) as c2\",\n",
    "            \"NTH (seq_product_id, 3 OF A) as c3\",\n",
    "            \"NTH (seq_product_id, 4 OF A) as c4\",\n",
    "            \"NTH (seq_product_id, 5 OF A) as c5\",\n",
    "            \"NTH (seq_product_id, 6 OF A) as c6\",\n",
    "            \"NTH (seq_product_id, 7 OF A) as c7\",\n",
    "            \"NTH (seq_product_id, 8 OF A) as c8\",\n",
    "            \"NTH (seq_product_id, 9 OF A) as c9\",\n",
    "            \"NTH (seq_product_id, 10 OF A) as c10\",\n",
    "            \"NTH (seq_product_id, 11 OF A) as c11\",\n",
    "            \"NTH (seq_product_id, 12 OF A) as c12\",\n",
    "            \"NTH (seq_product_id, 13 OF A) as c13\",\n",
    "            \"NTH (seq_product_id, 14 OF A) as c14\",\n",
    "            \"NTH (seq_product_id, 15 OF A) as c15\",\n",
    "            \"NTH (seq_product_id, 16 OF A) as c16\",\n",
    "            \"NTH (seq_product_id, 17 OF A) as c17\",\n",
    "            \"NTH (seq_product_id, 18 OF A) as c18\",\n",
    "            \"NTH (seq_product_id, 19 OF A) as c19\",\n",
    "            \"NTH (seq_product_id, 20 OF A) as c20\",\n",
    "            \"NTH (seq_product_id, 21 OF A) as c21\",\n",
    "            \"NTH (seq_product_id, 22 OF A) as c22\",\n",
    "            \"NTH (seq_product_id, 23 OF A) as c23\",\n",
    "            \"NTH (seq_product_id, 24 OF A) as c24\",\n",
    "            \"NTH (seq_product_id, 25 OF A) as c25\",\n",
    "    ]\n",
    ").result\n",
    "prepared_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b76d0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The aggregate function `NTH` is employed to extract the `seq_product_id` from each row in the sequence (where each product added to an order corresponds to a row in the sequence). This ID is then allocated to the appropriate column based on its position within the specific order. This is accomplished by specifying the indices `1` to `25` in the snippet above. If an order does not contain a product added at a particular position, we encounter a `None` value. These null values will need to be addressed and cleaned in the subsequent steps. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23176c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>At this point, we simply need to manage the null values and add end-of-sentence markers. We will leverage Teradata's robust in-database capabilities to accomplish this.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e0212",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As a first step we will persist the `prepared_ds` `DataFrame` to the data warehouse.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a4d6e-cebd-4a27-a93f-916b8f49365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds.to_sql(\"prepared_ds\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b074e6f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As a next step we need to produce a table with the following characteristics:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We need to insert the final sentence marker `102` after the last product added to each order. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The value of the columns where no product is placed instead of `null` should be `0`. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To achieve the conditions above, we need to inspect if the value of the column is `null` and modify that `null` conditionally, based on whether the preceding position is a valid product, in which case the ‘null’ should be replaced by an end of “sentence” marker, or another `null` value, in which case it should be replaced by `0`. </p> \n",
    "<p style = 'font-size:16px;font-family:Arial'>The COALESCE function enables us to efficiently apply a condition when we encounter that the value of a column is `null`. In this context, it is used to inspect each column, and if the column's value is `null`, it then checks the preceding column's value using a CASE statement. The CASE statement allows for conditional logic in SQL. In this situation, if the previous column's value is `null`, it returns `0`; otherwise, it returns `102`.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We can utilize the following script to preserve a cleaned version that is prepared for the training phase.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe68774-d7f2-4db9-881b-5a3e943f65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cleaned_ds_qry = '''\n",
    "CREATE TABLE cleaned_ds AS (\n",
    "  SELECT\n",
    "    c0,\n",
    "    c1,\n",
    "    COALESCE(c2, CASE WHEN c1 IS NULL THEN 0 ELSE 102 END) AS c2,\n",
    "    COALESCE(c3, CASE WHEN c2 IS NULL THEN 0 ELSE 102 END) AS c3,\n",
    "    COALESCE(c4, CASE WHEN c3 IS NULL THEN 0 ELSE 102 END) AS c4,\n",
    "    COALESCE(c5, CASE WHEN c4 IS NULL THEN 0 ELSE 102 END) AS c5,\n",
    "    COALESCE(c6, CASE WHEN c5 IS NULL THEN 0 ELSE 102 END) AS c6,\n",
    "    COALESCE(c7, CASE WHEN c6 IS NULL THEN 0 ELSE 102 END) AS c7,\n",
    "    COALESCE(c8, CASE WHEN c7 IS NULL THEN 0 ELSE 102 END) AS c8,\n",
    "    COALESCE(c9, CASE WHEN c8 IS NULL THEN 0 ELSE 102 END) AS c9,\n",
    "    COALESCE(c10, CASE WHEN c9 IS NULL THEN 0 ELSE 102 END) AS c10,\n",
    "    COALESCE(c11, CASE WHEN c10 IS NULL THEN 0 ELSE 102 END) AS c11,\n",
    "    COALESCE(c12, CASE WHEN c11 IS NULL THEN 0 ELSE 102 END) AS c12,\n",
    "    COALESCE(c13, CASE WHEN c12 IS NULL THEN 0 ELSE 102 END) AS c13,\n",
    "    COALESCE(c14, CASE WHEN c13 IS NULL THEN 0 ELSE 102 END) AS c14,\n",
    "    COALESCE(c15, CASE WHEN c14 IS NULL THEN 0 ELSE 102 END) AS c15,\n",
    "    COALESCE(c16, CASE WHEN c15 IS NULL THEN 0 ELSE 102 END) AS c16,\n",
    "    COALESCE(c17, CASE WHEN c16 IS NULL THEN 0 ELSE 102 END) AS c17,\n",
    "    COALESCE(c18, CASE WHEN c17 IS NULL THEN 0 ELSE 102 END) AS c18,\n",
    "    COALESCE(c19, CASE WHEN c18 IS NULL THEN 0 ELSE 102 END) AS c19,\n",
    "    COALESCE(c20, CASE WHEN c19 IS NULL THEN 0 ELSE 102 END) AS c20,\n",
    "    COALESCE(c21, CASE WHEN c20 IS NULL THEN 0 ELSE 102 END) AS c21,\n",
    "    COALESCE(c22, CASE WHEN c21 IS NULL THEN 0 ELSE 102 END) AS c22,\n",
    "    COALESCE(c23, CASE WHEN c22 IS NULL THEN 0 ELSE 102 END) AS c23,\n",
    "    COALESCE(c24, CASE WHEN c23 IS NULL THEN 0 ELSE 102 END) AS c24,\n",
    "    CASE WHEN c25 IS NULL THEN 0 ELSE 102 END AS c25\n",
    "  FROM prepared_ds\n",
    ") WITH DATA;\n",
    "'''\n",
    "eng.execute(create_cleaned_ds_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3643e8f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The resulting data would look like this:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672f38e-774f-4f8c-8e4f-93cd71ddd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds_dtf = tdml.DataFrame('cleaned_ds')\n",
    "cleaned_ds_dtf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8c3aa",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The cleaned training dataset can be persisted to object storage to easily be consumed by any Machine Learning tool on any cloud. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We could easily store the data to a parquet file in Azure, for example, with a simple statement like this:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ce327-8ab8-45ac-9bdf-02172d9a7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SELECT NodeId, AmpId, Sequence, ObjectName, ObjectSize, RecordCount\n",
    "FROM WRITE_NOS_FM (\n",
    "    ON (\n",
    "        select * from cleaned_ds\n",
    "    )\n",
    "\n",
    "    USING\n",
    "    LOCATION('/AZ/<azure_blob_storage_folder>/')\n",
    "    STOREDAS('PARQUET')\n",
    "    COMPRESSION('GZIP')\n",
    "    NAMING('RANGE')\n",
    "    INCLUDE_ORDERING('TRUE')\n",
    "    MAXOBJECTSIZE('4MB')\n",
    ") AS d \n",
    "ORDER BY AmpId;\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4cc46",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Model Training</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We are prepared to begin training a model now. Teradata can easily be integrated with Azure ML, especially if you already have a Teradata Vantage Cloud Lake Instance in the platform. Integration is also very convenient with AWS Sage Maker and Google Vertex. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the example we have been discussing the data used for training consists of a sequence of products. The training stage is all about discovering the relationship between these products as regard to the context in which they were added to the shopping cart. This is analogous to discovering the order of words in a sentence based on the context. The vector space in which the products, `words` are assigned a value based on their relation to each other is called an embedding. And the type of neural network used to discover these types of relations is called a transformer.</p> \n",
    "<p style = 'font-size:16px;font-family:Arial'>Since neural networks go about discovering the relationships by masking some values in the sentence and trying to guess what ‘word’ should occupy that space, the process is computationally expensive.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Training in the cloud offers the advantage of scalability and cost-performance since compute resources can be engaged when they are cheapest, and to the amount that is strictly needed. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>During training the predictive capability of the model is continually tested. For this purpose, usually the available data is divided between a training set, used to train the model, and a testing set. When the predictive capability of the model is deemed good enough, in accordance with the business requirements, and business and technical constraints the model is ready for operationalization.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In our case, once trained, our language model can be utilized to carry out tasks like generating a sequence of products, which can then serve as recommendations for consumers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20117a10",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Model Deployment</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the past it was common to build services around the environment on which the model was built. These services usually exposed endpoints to request predictions from the model. Under this type of architecture, the processes for maintaining the model, adjustment, train on new results, etc., and the efficiency of the systems depending on it was not optimal. Replicating the entirety of an environment is not straightforward. For this reason, standards, such as ONNX, have been developed to export the model configuration easily. It is rather common to export these files to an independent service and build some API endpoints around that system. However, this only solves part of the problem, the model is still isolated from the business context. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata Vantage incorporates the Build Your Own Model package (BYOM). BYOM allows to import the model, as an ONNX file, for example, into a table in the data warehouse/ lake. The model becomes, under this paradigm, another asset in your data environment, that you can operationalize, update, and refine.\n",
    "The deployment of a model in this scenario could be something as simple as the code below, you identify your model, you identify the table that you want to export it and voila!</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a122e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy downloaded model to Teradata\n",
    "print(f'Deploying model with id \"%s\" to table \"%s\"...' % (deployment_conf[\"model_id\"], deployment_conf['model_table']))\n",
    "tdml.delete_byom(\n",
    "    deployment_conf[\"model_id\"],\n",
    "    deployment_conf['model_table'])\n",
    "tdml.save_byom(\n",
    "    deployment_conf[\"model_id\"], \n",
    "    './' + model_file_path,\n",
    "    deployment_conf['model_table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fe9e3",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Model Operations</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the context of our example, we could run very straight forward SQL commands to retrieve recommendations from the model deployed in our data warehouse/lake. This has the added advantage that the recommendations from the model could further be refined with context from other data in our data environment, previous history of the specific costumer, local offerings according to the customer location, and many other parameters. This is an advantage that an isolated generative AI model would not be able to integrate.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0a26f",
   "metadata": {},
   "source": [
    "<img id=\"architecture-diagram\" src=\"./Images/architecture.jpg\" alt=\"architecture\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce643d9a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Reference query for retrieving recommendations</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We use VantageCloud Lake function ONNXPredict</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc077c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_based_recommendations(prouct_names, rec_number = 5, overwrite_cache = False):\n",
    "    seq_ids = collect_seq_ids(prouct_names)\n",
    "    select_tensor = generate_select_tensor(seq_ids, 32)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    select tokennum as num, product_name, p.product_id, p.department_id\n",
    "    from\n",
    "    (\n",
    "        with tbl as\n",
    "        (\n",
    "            select\n",
    "                REGEXP_SUBSTR(json_report,'([0-9,]+)', 1, 1, 'c') score\n",
    "            from\n",
    "                mldb.ONNXPredict(\n",
    "                on (%s)\n",
    "                    on (select * from %s where model_id = '%s') dimension\n",
    "                    using\n",
    "                        Accumulate('input_0_0')\n",
    "                        %s\n",
    "                ) a\n",
    "            )\n",
    "        SELECT \n",
    "            tokennum, \n",
    "            cast(seq_product_id as int) seq_product_id\n",
    "        FROM TABLE (STRTOK_SPLIT_TO_TABLE(1, tbl.score, ',')\n",
    "            RETURNS (outkey INTEGER,\n",
    "                    tokennum INTEGER,\n",
    "                    seq_product_id VARCHAR(30) CHARACTER SET UNICODE)\n",
    "                ) AS d\n",
    "                where tokennum <= %d\n",
    "        ) f\n",
    "        join product_id_to_seq_product_id pitspi on pitspi.seq_product_id = f.seq_product_id\n",
    "        join products p on p.product_id = pitspi.product_id\n",
    "    \"\"\"%(\n",
    "        select_tensor, \n",
    "        deployment_conf[\"model_table\"], \n",
    "        deployment_conf[\"model_id\"], \n",
    "        f\"OverwriteCachedModel('%s')\"%deployment_conf[\"model_id\"] if overwrite_cache else \"\",\n",
    "        rec_number\n",
    "        )\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    sql_res = conn.execute(query)\n",
    "\n",
    "    for row in sql_res:\n",
    "        result[row[\"num\"]] = {\"product_name\": row[\"product_name\"], \"product_id\": row[\"product_id\"], \"department_id\": row[\"department_id\"], \"aisle_id\": row[\"aisle_id\"]}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b2d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
